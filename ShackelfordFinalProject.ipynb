{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daniel Shackelford\n",
    "\n",
    "CSPB3202 FinalProject\n",
    "\n",
    "Github: https://github.com/dash6877/CSPB3202FinalProject.git\n",
    "\n",
    "Video:\n",
    "\n",
    "\n",
    "    Non-trained model:\n",
    "    https://gifyu.com/image/tyut\n",
    "    \n",
    "    Trained model:\n",
    "    https://gifyu.com/image/tyuO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Short overview:***\n",
    "\n",
    "For my final project, I decided to analyze the LunarLander-v2 gym environment using a Deep Q Network. The Lunar Lander task is centered around making movements to land a space ship inside of a designated area. The Lunar Lander receives rewards based on how close to the landing pad the lander gets without crashing. \n",
    "\n",
    "The goal of this specific project and implementation is to find a solution to the model in as few episodes as possible. The environment was tested with a general random approach and then tested again using a deep Q network. After implementing the network, it was also optimized. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn #neural networks\n",
    "import torch.nn.functional as Functional \n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "from collections import namedtuple, deque #stack\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize\n",
    "env=gym.make('LunarLander-v2')\n",
    "env.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Approach:***\n",
    "\n",
    "The state of the environment has 8 continuous values that represent the position of the lander in the x,y space, its velocity, angular speed, orientation, and other. There are 4 actions associated with the lander, these being: do nothing, fire left rocket, fire main rocket, and fire right rocket. The landing pad will always be at coordinates (0,0). Rewards for moving from the top of the screen to the bottom of the screen and at a speed of 0 is from 100-140. The amount of this reward is directly proportional to how close the lander is to the landing pad. The lander is given a +10 value for each leg that is on the ground, and firing the main engine incurs a reward of -0.3. The amount of fuel is infinite over the entire simulation. From a solutions standpoint the main engine firing penalty is an order of magnitude smaller than any of the rewards so, in general, a model that fires the main engine a lot is not necessarily a 'bad' solution. \n",
    "\n",
    "In the end, a deep q-learning network was selected for analysis of the system. The gym environement provides us with a number of states and actions. When the action space is discrete and the state space is continuous, DQN's work incredibly well. DQN's are also especially suited to overcome unstable learning using techniques like experience replay and target networks. The network used in this project was a Q network that included 3 fully connected layers, 'l1' being the input layer that takes in a tesnor of our state size and outputs a tensor that is the size of the hidden nodes. The 'l3' layers takes an input of our hidden nodes and outputs a tensor that is the size of our action space. This outputs an action for each possible action that our agent can take. Torch also requires a forward method for the neural network library. The foward method takes in a tensor 'x' that is equal to the game state the agent is observing. This gets passed through the first layer of the neural network and we apply the ReLU activation function. We then do the same thing with the second layer. Then that value gets passed into the third layer as the output of the whole network. The forward method in torch is automatically used when data is passed into the network. I attempted to implement dropout inside of the network, however it greatly reduced the rate at which the model is trained and the model, without dropout did not exhibit signs of overfitting the data set.\n",
    "\n",
    "The Agent in question utilizes two instances of the QNet object, one of which is a training set and one of which is a target set. In regards to action selection, the agent uses an epsilon-greedy algorithm to select actions. This is a straightforward algorithm and can be replaced in the future to increase efficiency. The main reason for this is the use of a soft update function. Soft update was initialized inside of the model as it is a recommended method for improving performance of our data set. The problem with standard Q learning is that at any given point in time in the analysis of the system, the current value and the goal state use the same network weights. Every time the system learns, the weight of the Q-network changes. Since the goal also changes, covnergence on a certain value becomes harder. To remedy this issue, we can create a separate network, called a target network. A target network maintains a fixed value during the learning process fo the original Q-network, and then periodically resets the q-network to its original q-network value (This concept was taken from the deepmind paper listed below). The soft update allows for the target network to be updated bit by bit, frequently over the coruse of the simulation. The value tau is used to control this change. This allows the target network to move slightly to the value of the Q-network. Because tao is small, the updates need to be frequent. Prior to running the model, another quick way to gain some extra performance is to utilize proper initialization parameters in our model. We will delve into which parameters helped the most over the course of testing in our results section.\n",
    "\n",
    "The model is trained using the 'DQN' function, whereby it is given a certain number of episodes to test over, and the model tries to receive the largest reward. For the purposes of this project, the testing will essentially never reach the maximum number of episodes as long as the model we are using is 'good enough'. This is because we set a hard limit of the model to stop once we receive a mean value that is greater than 200 from the reward function. This is because a value of 200 is the value assigned in the simulation to a correct solution, so maximization beyond this hard cutoff is essentially pointless. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state is of the form:  (8,)\n",
      "The total number of actions:  4\n"
     ]
    }
   ],
   "source": [
    "print (\"The state is of the form: \", env.observation_space.shape)\n",
    "print (\"The total number of actions: \", env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "useCuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARAMETERS:\n",
    "bufferSize = int(100000) # replay buffer size\n",
    "batchSize = 64 # minibatch size\n",
    "gammaDF = 0.99 # discount factor\n",
    "tau = 0.001 # for soft update\n",
    "learningRate= 0.0005 # learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, stateSize, actionSize, seed):\n",
    "        super(QNet, self).__init__()\n",
    "        \n",
    "        self.seed = torch.manual_seed(seed) #have to set seed generation for replication\n",
    "\n",
    "        self.l1 = nn.Linear(stateSize, 64) # create three nn layers from torch and set dimensions (64 for layers)\n",
    "#       self.dropout1 = nn.Dropout(p=0.1)\n",
    "        self.l2 = nn.Linear(64, 64)\n",
    "#       self.dropout2 = nn.Dropout(p=0.1)\n",
    "        self.l3 = nn.Linear(64, actionSize) #output\n",
    "    \n",
    "    def forward(self, state): #standard torch setup\n",
    "        x = self.l1(state)\n",
    "#       x = self.dropout1(x)\n",
    "        x = Functional.relu(x)\n",
    "        x = self.l2(x)\n",
    "#       x = self.dropout1(x)\n",
    "        x = Functional.relu(x)\n",
    "        action = self.l3(x)\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayExperience:\n",
    "\n",
    "    def __init__(self, actionSize, bufferSize, batchSize, seed): #init experience store object\n",
    "        self.actionSize = actionSize #dimension of action\n",
    "        self.memory = deque(maxlen=bufferSize) #stack mem with max buffer size\n",
    "        self.batchSize = batchSize #size of training batch\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"nextState\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, nextState, done): #add to mem stack\n",
    "        exp = self.experience(state, action, reward, nextState, done)\n",
    "        self.memory.append(exp)\n",
    "    \n",
    "    def sample(self): #get a batch of experiences and assign var values\n",
    "        experiences = random.sample(self.memory, self.batchSize) #from mem, grab batchSize number of items\n",
    "        \n",
    "        #get each of the vals in the experiences for the batch and return\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        nextStates = torch.from_numpy(np.vstack([e.nextState for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device) \n",
    "  \n",
    "        return (states, actions, rewards, nextStates, dones)\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.memory)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, stateSize, actionSize, seed):\n",
    "        self.stateSize = stateSize #state dimensions\n",
    "        self.actionSize = actionSize #action dimensions\n",
    "        self.seed = random.seed(seed) \n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetLocal = QNet(stateSize, actionSize, seed).to(device) #will test on local \n",
    "        self.qnetTarget = QNet(stateSize, actionSize, seed).to(device) #will save off target\n",
    "        self.optimizer = optim.Adam(self.qnetLocal.parameters(), lr=0.0005) #optimize with Adam\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayExperience(actionSize, bufferSize, batchSize, seed)\n",
    "\n",
    "    def step(self, state, action, reward, nextState, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, nextState, done)\n",
    "        if len(self.memory) > batchSize:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, gammaDF)\n",
    "\n",
    "\n",
    "    def act(self, state, epsilon=0.): #return action for state\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetLocal.eval()\n",
    "        action_values = self.qnetLocal(state)\n",
    "        self.qnetLocal.train()\n",
    "        # Epsilon-greedy action selection \n",
    "\n",
    "        if random.random() <= epsilon:    \n",
    "            return random.choice(np.arange(self.actionSize))\n",
    "\n",
    "        else:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "\n",
    "    def learn(self, experiences, gammaDF): #update value parameters using softUpdate \n",
    "        states, actions, rewards, nextStates, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        QNext = self.qnetTarget(nextStates).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states \n",
    "        Qtargets = rewards + (gammaDF * QNext * (1 - dones))\n",
    "        # Get expected Q values from local model\n",
    "        Qexpected = self.qnetLocal(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = Functional.mse_loss(Qexpected, Qtargets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad() #set gradients on Adam optimizer to zero\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        #update target\n",
    "        self.softUpdate(self.qnetLocal, self.qnetTarget, tau)\n",
    "        \n",
    "    # SoftUpdate idea from: https://greentec.github.io/reinforcement-learning-third-en/#soft-update-target-network\n",
    "    def softUpdate(self, localModel, targetModel, tau):\n",
    "        for targetParam, localParam in zip(targetModel.parameters(), localModel.parameters()):\n",
    "            \n",
    "            targetParam.data.copy_(tau*localParam.data + (1.0-tau)*targetParam.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Untrained Agent\n",
    "agent = Agent(8, 4, 0)\n",
    "# watch an untrained agent\n",
    "state = env.reset()\n",
    "for i in range(3):\n",
    "    state = env.reset()\n",
    "    for j in range(500):\n",
    "        action = agent.act(state)\n",
    "        env.render()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as above, used to create a gif for submission--does not have to be run every time. \n",
    "from PIL import Image\n",
    "\n",
    "agent = Agent(8, 4, 0)\n",
    "# watch an untrained agent\n",
    "state = env.reset()\n",
    "frames=[]\n",
    "for i in range(5):\n",
    "    \n",
    "    state = env.reset()\n",
    "    for j in range(500):\n",
    "        action = agent.act(state)\n",
    "        frames.append(Image.fromarray(env.render(mode='rgb_array')))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break \n",
    "    env.close()\n",
    "\n",
    "with open('./preTrain.gif', 'wb') as foo:\n",
    "    im = Image.new('RGB', frames[0].size)\n",
    "    im.save(foo, save_all=True, append_images=frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://gifyu.com/image/tyut\n",
    "\n",
    "#UNCOMMENT BELOW AND RUN TO LOAD GIF...\n",
    "# with open('./preTrain.gif','rb') as f:\n",
    "#     display(Image(data=f.read(), format='png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DQN learning! \n",
      "\n",
      "Episode:                Average Score: \n",
      "Episode: 100 --------> Average Score: -121.51\n"
     ]
    }
   ],
   "source": [
    "def DQN():\n",
    "    totalPossibleEpisodes=1000\n",
    "    epsilonDecay=0.995 #need to decrease epsilon until down for each episode\n",
    "    eps = 0.9 # initialize epsilon\n",
    "    \n",
    "    scores = [] # list containing scores from each episode\n",
    "    currentSet = deque(maxlen=100) # last 100 scores\n",
    "\n",
    "    print('Starting DQN learning! \\n')\n",
    "    print('Episode:                Average Score: ')\n",
    "    \n",
    "    for i in range(1, totalPossibleEpisodes+1):\n",
    "        state = env.reset()\n",
    "        currScore = 0\n",
    "        \n",
    "        for t in range(1000):#1000 is the cut off for time steps allowed per episode\n",
    "            action = agent.act(state, eps)\n",
    "            nextState, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, nextState, done)\n",
    "            state = nextState\n",
    "            currScore += reward\n",
    "            if done:\n",
    "                break \n",
    "                \n",
    "        currentSet.append(currScore) # save most recent score\n",
    "        scores.append(currScore) # save most recent score\n",
    "        eps = max(0.01, epsilonDecay*eps) # decrease epsilon\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('\\rEpisode: {} --------> Average Score: {:.2f}'.format(i, np.mean(currentSet)))\n",
    "        if np.mean(currentSet)>=200.0: #check to see if success\n",
    "            print('\\nEnvironment solved in {:d} episodes! --------> Average Score: {:.2f}'.format(i, np.mean(currentSet)))\n",
    "            torch.save(agent.qnetLocal.state_dict(), 'finalSolution.pth') #save the local qnet as final\n",
    "            break\n",
    "            \n",
    "    return scores\n",
    "\n",
    "scores = DQN()\n",
    "#roughly 385 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode Number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watch the smart agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights from file\n",
    "agent.qnetLocal.load_state_dict(torch.load('finalSolution.pth'))\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    state = env.reset()\n",
    "    for j in range(1000):\n",
    "        action = agent.act(state)\n",
    "        env.render()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "#Same as above, used to create a gif for submission--does not have to be run every time. \n",
    "agent.qnetLocal.load_state_dict(torch.load('finalSolution.pth'))\n",
    "\n",
    "frames2=[]\n",
    "for i in range(5):\n",
    "    state = env.reset()\n",
    "    for j in range(1000):\n",
    "        action = agent.act(state)\n",
    "        frames2.append(Image.fromarray(env.render(mode='rgb_array')))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "env.close()\n",
    "\n",
    "with open('./postTrain.gif', 'wb') as foo:\n",
    "    im = Image.new('RGB', frames2[0].size)\n",
    "    im.save(foo, save_all=True, append_images=frames2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE THIS TO SEE GIF\n",
    "#https://gifyu.com/image/tyuO\n",
    "\n",
    "#UNCOMMMENT BELOW LINES TO LOAD GIF INTO NOTEBOOK....\n",
    "# with open('./postTrain.gif','rb') as f:\n",
    "    \n",
    "#     display(Image(data=f.read(), format='png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Results:***\n",
    "\n",
    "Over the course of creation of this model, multiple different neural networks were attempted. The original network that was created utilized multiple 2D convolutional layers inside of its archetecture, however that model struggled with the continuous state of the model. \n",
    "\n",
    "The selected model has immediate benefit over other possible models as the tensor input and output of the linear model allows for better analysis of the continuous domain of the problem, as well as the RELU functions between layers. Once the model created, it took a large amount of trial and error in order to ascertain the correct starting parameters, namely reducing the starting epsilon value, changing the learning rate, and altering epsilon decay. The most straighforward and fastest jump in performance is to lessen the starting epsilon value. \n",
    "\n",
    "Once these changes had been implemented the model was run. The original model without improvements achieved a mean reward value above 200 in 787 episodes, and after improvements the new model achieved a mean reward value above 200 in 385 episodes. The reward for each episode in the set has been plotted above. It can be seen that although our reward per episode may be relatively noisy, the overall trend increases steadily over the course of the testing. Of note, we can see a slight leveling off of the reward increase from 150-275 episodes and again after 325 episodes. There is also a sharp increase between 275 and 325. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Conclusion:***\n",
    "\n",
    "Overall this project went well. The most difficult part of the entire project was learning how to properly implement an agent using pyTorch and doing research on DQNs and how they are implemented. I have listed the references my resources below, of particular note is the paper by deepmind that also uses many of these techniques. Moving forward in the project, I would like to implement a Double-deep q-learning network. David Silver of Deepmind pointed out that DDQN is one of the major improvements made since Nature DQN. DDQN does not suffer as much from an inherent issue with Q learning, the tendency for overestimation fo q-values. I think that this implementation would be very interesting to compare to the standard DQN solution found above. In conclusion, this project was incredibly informative and lead to me receiving a huge amount of knowledge in current RL algorithms and how they are used. I am excited that I had the opportunity to learn pyTorch and its applications and I had a lot fo fun implementing this simulation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REFERENCED MATERIAL:\n",
    "\n",
    "http://seba1511.net/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "https://greentec.github.io/reinforcement-learning-third-en/#soft-update-target-network\n",
    "\n",
    "https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\n",
    "\n",
    "https://arxiv.org/abs/1509.06461\n",
    "\n",
    "https://arxiv.org/abs/1511.06581\n",
    "\n",
    "https://arxiv.org/abs/1511.05952\n",
    "\n",
    "https://medium.com/coinmonks/landing-a-rocket-with-simple-reinforcement-learning-3a0265f8b58c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
